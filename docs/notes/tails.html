<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.56">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>On Tails of Distributions and Implications for Statistical Inference – STA 9715</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">STA 9715</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href=".././syllabus.html"> 
<span class="menu-text">Syllabus</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href=".././notes.html"> 
<span class="menu-text">Handouts and Additional Notes</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href=".././resources.html"> 
<span class="menu-text">Additional Resources</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href=".././objectives.html"> 
<span class="menu-text">Learning Objectives</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#on-the-behavior-of-sample-means" id="toc-on-the-behavior-of-sample-means" class="nav-link active" data-scroll-target="#on-the-behavior-of-sample-means">On the Behavior of Sample Means</a></li>
  <li><a href="#quantitative-bounds-on-random-deviations" id="toc-quantitative-bounds-on-random-deviations" class="nav-link" data-scroll-target="#quantitative-bounds-on-random-deviations">Quantitative Bounds on Random Deviations</a></li>
  <li><a href="#application-to-sample-means" id="toc-application-to-sample-means" class="nav-link" data-scroll-target="#application-to-sample-means">Application to Sample Means</a></li>
  <li><a href="#application-to-indicator-functions" id="toc-application-to-indicator-functions" class="nav-link" data-scroll-target="#application-to-indicator-functions">Application to Indicator Functions</a></li>
  <li><a href="#heavy-tails" id="toc-heavy-tails" class="nav-link" data-scroll-target="#heavy-tails">Heavy Tails</a>
  <ul class="collapse">
  <li><a href="#whence-heavy-tails" id="toc-whence-heavy-tails" class="nav-link" data-scroll-target="#whence-heavy-tails">Whence Heavy Tails?</a></li>
  <li><a href="#heavy-tails-in-finance" id="toc-heavy-tails-in-finance" class="nav-link" data-scroll-target="#heavy-tails-in-finance">Heavy Tails in Finance</a></li>
  </ul></li>
  <li><a href="#degrees-of-heaviness" id="toc-degrees-of-heaviness" class="nav-link" data-scroll-target="#degrees-of-heaviness">Degrees of Heaviness</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">On Tails of Distributions and Implications for Statistical Inference</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p><span class="math display">\[\newcommand{\P}{\mathbb{P}} \newcommand{\E}{\mathbb{E}} \newcommand{\V}{\mathbb{V}}\]</span></p>
<p>In this set of notes, we introduce the concepts of the “tails” of a distribution and discuss implications for statistical inference.</p>
<p>The discussion of distribution tails is typically dominated by talk of “fat” or “heavy” tails. Heavy-tailed models are particularly popular in post-2008 finance, for reasons we will discuss below, as they naturally allow “crash” behvaior. Before we turn to heavy-tails, however, we need to discuss “standard” tails and their implications for statistical inference.</p>
<p>Much of statistical theory is built on a foundation of “tail bounds”. Tail bounds give an upper bound on the probability of some random variable being greater than some quantity: a prototypical tail bound is something like “the probability of procedure <span class="math inline">\(\mathscr{P}\)</span> having error more than <span class="math inline">\(\delta\)</span> is less than <span class="math inline">\(\epsilon\)</span>”, where <span class="math inline">\(\delta\)</span> and <span class="math inline">\(\epsilon\)</span> are both some small numbers. For instance, a predictive model may guarantee that the probability of being off by more than 2 standard deviations is less than 5% or, equivalently, that 95% of observations will fall within <span class="math inline">\(2\sigma\)</span> of the predicted value. Tail bounds can also be used to build confidence intervals: a tail bound of the form “the probability that the CI doesn’t contain the true value is less than 5% under the null” is essentially the definition of a confidence interval. Here, the error is binary (0 vs 1) but the basic machinery still holds.</p>
<p>In all these cases, we are constructing some random variable - the procedure error - and hoping to guarantee that it’s not too large. Typically, the procedure error is a (not-too-complex) function of several independent sources of ‘noise’ or error. The output of the procedure is then itself random<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>, as is its error. Rather counterintuitively, even if the error is a function of <em>many</em> random components, the aggregate effect of that error is usually quite predictable. This is a phenomenon we’ll see in many forms throughout this course, so let’s begin to explore it properly.</p>
<section id="on-the-behavior-of-sample-means" class="level3">
<h3 class="anchored" data-anchor-id="on-the-behavior-of-sample-means">On the Behavior of Sample Means</h3>
<p>Let’s start with the most fundamental and most well studied problem in all of statistics: estimating the mean of a distribution. Suppose we have a single sample <span class="math inline">\(X\)</span> from some distribution. Without making any further assumptions on <span class="math inline">\(X\)</span>, we only assume it has a CDF <span class="math inline">\(F_X(\cdot)\)</span>, so we write <span class="math inline">\(X \sim F_X(\cdot)\)</span>. Let’s also assume that <span class="math inline">\(X\)</span> (or perhaps, more precisely, <span class="math inline">\(F_X(\cdot)\)</span>) has a mean <span class="math inline">\(\mu_X\)</span> and a variance <span class="math inline">\(\sigma_X^2\)</span>, even if we are not yet able to compute them.</p>
<p>Since we are now in “statistics-mode”<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> and have set ourselves the task of guessing <span class="math inline">\(\mu_X\)</span>, we need to define our “guessing procedure,” which in statistics jargon we call our “estimator”. It’s important to distinguish three very similar words in this context:</p>
<ul>
<li>The “Estimand” is the (population/distribution) quantity we are ultimately interested in, here <span class="math inline">\(\mu_X\)</span>.</li>
<li>The “Estimator” is the <em>procedure</em> we use to guess at the estimand. It takes in data (observations) and produces a value, so it is a <em>function</em>, not a number.</li>
<li>The “Estimate” is the value of the estimator on a particular data set; that is, it is the <em>output</em> of the estimator function.</li>
</ul>
<p>In our simple case, our <em>estimand</em> is <span class="math inline">\(\mu_X\)</span> and we need to define an <em>estimator</em>. Under the “when in doubt, average it out” rule of statistics, let’s define the sample mean <span class="math display">\[\overline{X}_n = \textsf{MEAN}(\{X_1, \dots, X_n\}) = \frac{1}{n}\sum_{i=1}^n X_i\]</span> as our estimator.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> By linearity of expectation, we see that</p>
<p><span class="math display">\[\E[\overline{X}_n] = \E\left[\frac{1}{n}\sum_{i=1}^n X_i\right] = \frac{1}{n}\sum_{i=1}^n \E[X_i] = \frac{1}{n}\sum_{i=1}^n \mu_X = \mu_X\]</span></p>
<p>This means that, <em>on average</em>, <span class="math inline">\(\overline{X}_n\)</span> gets the ‘right’ answer of <span class="math inline">\(\mu_X\)</span>. In statistics, we thus say that <span class="math inline">\(\overline{X}_n\)</span> is an <em>unbiased</em> estimator of <span class="math inline">\(\mu_X\)</span>.<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> So, on average, <span class="math inline">\(\overline{X}_n\)</span> is a pretty solid guess - but how well does it do on a <em>single</em> data set? Recall, as before, that this is our fundamental characterization of variance.<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a></p>
<p><span class="math display">\[\E[\text{MeanSquaredError}(\overline{X}_n, \mu_X)] = \E[(\overline{X}_n - \mu_X)^2] = \V[\overline{X}_n]\]</span></p>
<p>So the lower the variance of <span class="math inline">\(\overline{X}_n\)</span>, the better an estimator it is. So what actually is the variance of <span class="math inline">\(\overline{X}_n\)</span>? If we assume <span class="math inline">\(X_1, \dots, X_n \buildrel{\text{iid}}\over\sim F_X(\cdot)\)</span>, we have developed enough machinery to show that:</p>
<p><span class="math display">\[\begin{align*}
\V[\overline{X}_n] &amp;= \V\left[\frac{1}{n} \sum_{i=1}^n X_i\right] \\
&amp;= \frac{1}{n^2} \V\left[\sum_{i=1}^n X_i\right] \\
&amp;= \frac{1}{n^2} \left(\sum_{i=1}^n \V[X_i]\right) \\
&amp;= \frac{1}{n^2} \left(\sum_{i=1}^n \sigma_X^2\right) \\
&amp;= \frac{1}{n^2} * n\sigma_X^2 \\
&amp;= \frac{\sigma_X^2}{n}
\end{align*}\]</span></p>
<p>This is quite nifty! As we get more samples and <span class="math inline">\(n\)</span> grows, the variance of <span class="math inline">\(\overline{X}_n\)</span> gets smaller! Put another way, as <span class="math inline">\(n \to \infty\)</span>, <span class="math inline">\(\overline{X}_n\)</span> becomes increasingly “less random” until it settles down at the right answer of <span class="math inline">\(\mu_X\)</span>.</p>
<p>This phenomenon is important enough it gets a very cool sounding name:</p>
<div class="callout callout-style-default callout-tip callout-titled" title="The Law of Large Numbers">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The Law of Large Numbers
</div>
</div>
<div class="callout-body-container callout-body">
<p>Suppose <span class="math inline">\(X_1, X_2, \dots, X_n, \dots\)</span> are independent and identically distributed samples from a distribution with mean <span class="math inline">\(\mu_X\)</span> and variance <span class="math inline">\(\sigma_X^2\)</span>. The sample average <span class="math inline">\(\overline{X}_n = n^{-1}\sum_{i=1}^n X_i\)</span> converges to <span class="math inline">\(\mu_X\)</span>:<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a></p>
<p><span class="math display">\[ \overline{X}_n \to \mu_X\]</span></p>
</div>
</div>
</section>
<section id="quantitative-bounds-on-random-deviations" class="level3">
<h3 class="anchored" data-anchor-id="quantitative-bounds-on-random-deviations">Quantitative Bounds on Random Deviations</h3>
<p>So far, we have made the <em>qualitative</em> argument for a tail bound, but we don’t actually have any numbers to put on it. It’s clear that we want some version of “the probability of being at least <span class="math inline">\(k\)</span> standard deviations off the mean is less than <span class="math inline">\(f(k)\)</span>”, but where can we get such a bound?</p>
<p>We begin with an apparently unrelated result, commonly called Markov’s inequality.<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a></p>
<div class="callout callout-style-default callout-tip callout-titled" title="Markov's Inequality">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Markov’s Inequality
</div>
</div>
<div class="callout-body-container callout-body">
<p>Suppose <span class="math inline">\(X\)</span> is a non-negative random variable. Then for any <span class="math inline">\(a &gt; 0\)</span>,</p>
<p><span class="math display">\[\P(X &gt; a) \leq \frac{\E[X]}{a} \]</span></p>
</div>
</div>
<p>Note that this result requires <span class="math inline">\(X\)</span> to be non-negative. If <span class="math inline">\(X\)</span> were, <em>e.g.</em>, a standard normal random variable with support <span class="math inline">\((-\infty, \infty)\)</span>, it would give us <span class="math inline">\(\P(\mathcal{N}(0, 1) \geq 0) \leq 0\)</span>, when in fact the true value if <span class="math inline">\(\P(\mathcal{N}(0, 1) \geq 0) = \frac{1}{2}\)</span>.</p>
<p>It turns out to be quite easy to prove Markov’s inequality using only the law of total expectation:</p>
<p><span class="math display">\[\begin{align*}
\E[X] &amp;= \overbrace{\E[X | X \leq a]}^{\text{Value between $0$ and $a$}}\overbrace{\P(X \leq a)}^{\text{Non-Negative}} + \E[X | X &gt; a] \P(X &gt; a) \\
\implies \E[X] &amp;\geq \underbrace{\E[X | X &gt; a]}_{\text{Value greater than $a$}} \P(X &gt; a) \\
&amp;\geq a * \P(X &gt; a) \\
\implies \frac{\E[X]}{a} &amp; \geq P(X &gt; a)
\end{align*}\]</span></p>
<p>as desired.</p>
<p>This is quite a nifty result - we now have the ability to bound tail probabilities <span class="math inline">\(P(X &gt; a)\)</span> in terms of only the “tail point” <span class="math inline">\(a\)</span> and the (non-random, non-tail) expected value <span class="math inline">\(\E[X]\)</span>.</p>
<p>Looking a bit more closely at Markov’s inequality, we see that if <span class="math inline">\(\E[X]\)</span> is small, <span class="math inline">\(P(X &gt; a\)</span> can’t be very large. This makes sense, if there was a large probability that <span class="math inline">\(X\)</span> was larger than some value <span class="math inline">\(a\)</span>, that would have to bring up the value of <span class="math inline">\(\E[X]\)</span>. A small value of <span class="math inline">\(\E[X]\)</span> certifies that <span class="math inline">\(X\)</span> is “usually” small. Again - this is only true because <span class="math inline">\(X\)</span> is non-negative. If we allowed both positive and negative values in <span class="math inline">\(X\)</span>, they could both be large, but they would “net out” in computing <span class="math inline">\(\E[X]\)</span>.</p>
<p>Markov’s inequality is most useful when applied to functions of random variables. Suppose <span class="math inline">\(\phi(\cdot)\)</span> is a non-negative function. Then</p>
<p><span class="math display">\[\P(\phi(X) \geq a) \leq \frac{\E[\phi(X)]}{a} \]</span></p>
<p>for any random variable <span class="math inline">\(X\)</span>.</p>
<p>A particularly useful choice of <span class="math inline">\(\phi\)</span> is our old friend, squared error loss from mean prediction. Taking <span class="math inline">\(\phi(X) = (X - \E[X])^2\)</span>, Markov’s Inequality gives us Chebyshev’s Inequality:</p>
<p><span class="math display">\[ \P\left[(X - \E[X])^2 \geq a\right] \leq \frac{\E[(X - \E[X])^2]}{a} = \frac{\V[X]}{a} \]</span></p>
<p>Customarily, we take <span class="math inline">\(a = k^2\V[X]\)</span> to obtain:</p>
<p><span class="math display">\[\P\left[(X - \E[X])^2 \geq k^2\V[X]\right] \leq \frac{1}{k^2}\]</span></p>
<p>Taking square roots of both terms in the left side probability expression, we get an essentially uniform prediction bound:</p>
<div class="callout callout-style-default callout-tip callout-titled" title="Chebyshev's Inequality">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Chebyshev’s Inequality
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let <span class="math inline">\(X\)</span> be a random variable with mean <span class="math inline">\(\mu_X\)</span> and variance <span class="math inline">\(\sigma_X^2\)</span>. Then, for any <span class="math inline">\(k \geq 0\)</span>, we have the tail bound:</p>
<p><span class="math display">\[ \P\left[|X - \mu_X| \geq k \sigma_X\right] \leq \frac{1}{k^2} \]</span></p>
</div>
</div>
<p>This is quite remarkable. The “68%/95%/99%” set of bounds for the normal distribution are well known. Using Chebyshev, the same bounds are:</p>
<table class="caption-top table">
<caption>Comparison of Gaussian and Chebyshev Tail Bounds</caption>
<thead>
<tr class="header">
<th><span class="math inline">\(k\)</span></th>
<th>Prediction Interval</th>
<th>Gaussian Bound</th>
<th>Chebyshev Bound</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(k=1\)</span></td>
<td><span class="math inline">\(\mu_X \pm \sigma_X\)</span></td>
<td>68.2%</td>
<td>–</td>
</tr>
<tr class="even">
<td><span class="math inline">\(k=2\)</span></td>
<td><span class="math inline">\(\mu_X \pm 2\sigma_X\)</span></td>
<td>95.4%</td>
<td>75%</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(k=3\)</span></td>
<td><span class="math inline">\(\mu_X \pm 3\sigma_X\)</span></td>
<td>99.7%</td>
<td>88.8%</td>
</tr>
<tr class="even">
<td><span class="math inline">\(k=4\)</span></td>
<td><span class="math inline">\(\mu_X \pm 4\sigma_X\)</span></td>
<td>99.99%</td>
<td>93.75%</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(k=5\)</span></td>
<td><span class="math inline">\(\mu_X \pm 5\sigma_X\)</span></td>
<td>99.9999%</td>
<td>96%</td>
</tr>
</tbody>
</table>
<p>Our bounds are not as tight as we get if we know <span class="math inline">\(X\)</span> is Gaussian, but they hold for any random variable, so long as we know it’s mean and variance - no distributions needed!</p>
<p>These type of bounds - often called “non-parametric” - are quite popular in modern statistics and machine learning. When calibrating a complex machine learning system, <em>e.g.</em> a voice recognition system used to authenticate access to a sensitive banking system, there may be nothing “Gaussian” about the problem. But, so long as we have a numerical measure of accuracy with bounded mean and standard deviation, we can use Chebyshev to provide performance gurantees. In fact, because Chebyshev holds for <em>any</em> random variable, it is often quite pessimistic. If you use a Chebyshev bound, your clients will often be pleasantly surprised by how much better your model is than expected.</p>
</section>
<section id="application-to-sample-means" class="level3">
<h3 class="anchored" data-anchor-id="application-to-sample-means">Application to Sample Means</h3>
<p>Let’s now apply Chebyshev’s inequality to our previous example of the estimator <span class="math inline">\(\overline{X}_n\)</span>. Let’s now also assume that we know that <span class="math inline">\(\sigma_X = 1\)</span> to make some of the math a bit easier.<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a> Then, by our previous argument, we have <span class="math inline">\(\V[\overline{X}_n] = n^{-1}\)</span>. Plugging this into Chebyshev’s inequality, we have:</p>
<p><span class="math display">\[\P(|\overline{X}_n - \mu_X| &gt; k/n) \leq \frac{1}{k^2}\]</span></p>
<p>Equivalently, if we set <span class="math inline">\(k' = k/n\)</span> and <span class="math inline">\(k = nk'\)</span>, this gives us</p>
<p><span class="math display">\[
\P(|\overline{X}_n - \mu_X| &gt; k) \leq \frac{1}{k^2n^2}
\]</span></p>
<p>At some typical values of <span class="math inline">\(k, n\)</span>, let’s see what probabilities this gives us:</p>
<table class="caption-top table">
<caption>Chebyshev Bounds on Sample Mean Accuracy. <span class="math inline">\(\P(|\overline{X}_n - \mu_X| &gt; k) \leq 1/k^2n^2\)</span></caption>
<thead>
<tr class="header">
<th>Sample Size</th>
<th><span class="math inline">\(n=10\)</span></th>
<th><span class="math inline">\(n=50\)</span></th>
<th><span class="math inline">\(n=100\)</span></th>
<th><span class="math inline">\(n=1000\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(k=1\)</span></td>
<td>1%</td>
<td>0.04%</td>
<td>0.01%</td>
<td>0.0001%</td>
</tr>
<tr class="even">
<td><span class="math inline">\(k=2\)</span></td>
<td>0.25%</td>
<td>0.01%</td>
<td>0.0025%</td>
<td>0.000025%</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(k=4\)</span></td>
<td>0.0625%</td>
<td>0.0025%</td>
<td>0.00015625%</td>
<td>0.0000015625%</td>
</tr>
</tbody>
</table>
<p>This is great! Even with very small sample size <span class="math inline">\(n = 10\)</span>, we have 99% chance of being within one standard deviation of the true <span class="math inline">\(\mu_X\)</span>.</p>
</section>
<section id="application-to-indicator-functions" class="level3">
<h3 class="anchored" data-anchor-id="application-to-indicator-functions">Application to Indicator Functions</h3>
<p>A particularly useful set of random variables are our indicator functions</p>
<p><span class="math display">\[1_A(x) = \begin{cases} 1 &amp; x \in A \\ 0 &amp; x \notin A \end{cases}\]</span></p>
<p>Indicator functions are particularly useful because we can use them to turn guarantees on sample means into guarantees on probabilities. Recall the fundamental relationship:</p>
<p><span class="math display">\[\E[1_A(X)] = \P(X \in A)\]</span></p>
<p>Because indicator functions are also bounded, we can easily see that their variance is always bounded above by 1, unlike in our previous discussion (sample means) where we had to assume it was known.</p>
<p>To adapt our previous discussion, let</p>
<p><span class="math display">\[\P_n(X \in A) = \frac{1}{n} \sum_{i=1}^n 1_A(X_i) \]</span></p>
<p>be the <em>sample empirical probability</em> associated with a set <span class="math inline">\(A\)</span>. Clearly, we expect it to behave analogously to the sample mean. As before, we have that it is unbiased and has decaying variance:</p>
<p><span class="math display">\[\E[\P_n(X \in A)] = \P(X \in A) \]</span></p>
<p>and</p>
<p><span class="math display">\[\V[\P_n(X \in A)] \leq \frac{1}{n}\]</span></p>
<p>We can plug these into Chebyshev’s Inequality to get guarantees on the quality of <span class="math inline">\(\P_n(X \in A)\)</span> as an estimator of the estimand <span class="math inline">\(\P(X \in A)\)</span>. Put another way, we can show that, for any set <span class="math inline">\(A\)</span>, the sample probability converges to the “true” probability as <span class="math inline">\(n \to \infty\)</span>. The law of large numbers (combined with the frequency interpretation of probability) strikes again!</p>
<div class="callout callout-style-default callout-warning callout-titled" title="Practice">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Practice
</div>
</div>
<div class="callout-body-container callout-body">
<p>Apply Chebyshev’s Inequality to <span class="math inline">\(\P_n(X \in A)\)</span> to show that:</p>
<ol type="1">
<li><p>We can get good bounds on the error of the sample probability as an estimate of the true probability.</p></li>
<li><p>Refine the argument using <span class="math inline">\(\V[\text{Bernoulli}] \leq \frac{1}{4}\)</span> to get tighter bounds.</p></li>
</ol>
</div>
</div>
</section>
<section id="heavy-tails" class="level2">
<h2 class="anchored" data-anchor-id="heavy-tails">Heavy Tails</h2>
<p>So far, life is good. We have nice bounds that decay rapidly and hold with only minimal assumptions on <span class="math inline">\(X\)</span>. Sample means and sample probabilities do exactly what we want them to and, without too much work, we expect we could apply our techniques to other quantities of interest like medians and quartiles. What could go wrong?</p>
<p>The law of large numbers tells us that, with enough data, the randomness “washes out” from a large sample. But what happens if the randomness is <em>so wild</em> that it can’t be washed out? The LLN fails to hold and we are in a dark and scary place. This is the domain of heavy-tailed monsters.</p>
<p>The most famous heavy-tailed distribution is the <em>Cauchy</em> distribution, named after the French mathematician Augustin-Louis Cauchy and pronounced either Co-shee or Cow-shee depending on the quality of your French accent. It has a rather benign PDF and CDF:</p>
<p><span class="math display">\[\begin{align*}
f_X(x) &amp;= \frac{1}{\pi(1 + x^2)} \\
F_X(x) &amp;= \frac{1}{\pi}\arctan(x) + \frac{1}{2}
\end{align*}\]</span></p>
<p>Graphically, it looks quite similar to its normal distribution kin.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="at">length.out=</span><span class="dv">501</span>)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, <span class="fu">dnorm</span>(x), </span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">type=</span><span class="st">"l"</span>, <span class="at">col=</span><span class="st">"red4"</span>, <span class="at">lwd=</span><span class="dv">2</span>, </span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab=</span><span class="st">"x"</span>, <span class="at">ylab=</span><span class="fu">expression</span>(f[X](x)), </span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>     <span class="at">main=</span><span class="st">"Comparison of Normal (Red) and Cauchy (Green) PDFs"</span>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x, <span class="fu">dcauchy</span>(x), <span class="at">type=</span><span class="st">"l"</span>, <span class="at">col=</span><span class="st">"green4"</span>, <span class="at">lwd=</span><span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="tails_files/figure-html/unnamed-chunk-1-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="at">length.out=</span><span class="dv">501</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, <span class="fu">pnorm</span>(x), </span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">type=</span><span class="st">"l"</span>, <span class="at">col=</span><span class="st">"red4"</span>, <span class="at">lwd=</span><span class="dv">2</span>, </span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab=</span><span class="st">"x"</span>, <span class="at">ylab=</span><span class="fu">expression</span>(f[X](x)), </span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>     <span class="at">main=</span><span class="st">"Comparison of Normal (Red) and Cauchy (Green) CDFs"</span>)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x, <span class="fu">pcauchy</span>(x), <span class="at">type=</span><span class="st">"l"</span>, <span class="at">col=</span><span class="st">"green4"</span>, <span class="at">lwd=</span><span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="tails_files/figure-html/unnamed-chunk-2-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Eyeballing these graphs, we can see that the normal distribution has “more” mass near (<span class="math inline">\(x\)</span> near 0) and the Cauchy has more mass in the tails (<span class="math inline">\(x\)</span> far from 0), but the differences don’t appear that large.</p>
<p>But looks can deceive! The Cauchy distribution is quite a wild thing: it in fact fails to have an expected value. Mathematically, the integral</p>
<p><span class="math display">\[\E_{X \sim \text{Cauchy}}[X] = \int_{-\infty}^{\infty} \frac{x}{\pi(1+x^2)}\,\text{d}x\]</span></p>
<p>is ill-posed. To see this formally, note that the integrand is approximately <span class="math inline">\(1/x\)</span> so the indefinite integral is <span class="math inline">\(\ln(x)\)</span> which goes to <span class="math inline">\(\infty\)</span>.</p>
<p>But what does this actually mean? We first note that, if <span class="math inline">\(\E[X]\)</span> does not exist, neither does <span class="math inline">\(\V[X]\)</span>.<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a> From our discussion above, this means we have no law of large numbers. It’s easiest to see the impact of this visually.</p>
<p>Let’s start by simulating large numbers of normal random variables and plotting their cumulative means:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">500</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>r <span class="ot">&lt;-</span> <span class="dv">500</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>XX <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(n <span class="sc">*</span> r), <span class="at">nrow=</span>r)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>XX <span class="ot">&lt;-</span> <span class="fu">apply</span>(XX, <span class="dv">2</span>, dplyr<span class="sc">::</span>cummean)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="dv">1</span><span class="sc">:</span>n, XX[,<span class="dv">1</span>], <span class="at">type=</span><span class="st">"n"</span>, </span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab=</span><span class="st">"n"</span>, <span class="at">ylab=</span><span class="fu">expression</span>(<span class="fu">bar</span>(X[n])), </span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylim =</span> <span class="fu">range</span>(XX),</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>     <span class="at">main=</span><span class="st">"Law of Large Numbers - Normal Deviates"</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>r){</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>    <span class="fu">lines</span>(<span class="dv">1</span><span class="sc">:</span>n, XX[,i], <span class="at">col=</span>scales<span class="sc">::</span><span class="fu">alpha</span>(<span class="st">"blue2"</span>,<span class="fl">0.15</span>))</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(<span class="dv">1</span><span class="sc">:</span>n, <span class="sc">+</span><span class="dv">1</span><span class="sc">/</span><span class="fu">sqrt</span>(<span class="dv">1</span><span class="sc">:</span>n), <span class="at">col=</span><span class="st">"red4"</span>, <span class="at">lty=</span><span class="dv">2</span>)</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(<span class="dv">1</span><span class="sc">:</span>n, <span class="sc">-</span><span class="dv">1</span><span class="sc">/</span><span class="fu">sqrt</span>(<span class="dv">1</span><span class="sc">:</span>n), <span class="at">col=</span><span class="st">"red4"</span>, <span class="at">lty=</span><span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="tails_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Here, each line is a different ‘universe’ in which we observe <span class="math inline">\(\overline{X}_n\)</span> for different values of <span class="math inline">\(n\)</span>. For small values of <span class="math inline">\(n\)</span> (left hand side of the figure), there is a large variance, but as <span class="math inline">\(n \to \infty\)</span> and we move right, the variance washes out quickly. In fact, it washes out at a rate of <span class="math inline">\(1/\sqrt{n}\)</span> consistent with our theory for <span class="math inline">\(\sigma_{\overline{X}_n} = \sqrt{\V[\overline{X}_n]}\)</span>.</p>
<p>Contrast this with the Cauchy distribution:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">500</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>r <span class="ot">&lt;-</span> <span class="dv">500</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>XX <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rcauchy</span>(n <span class="sc">*</span> r), <span class="at">nrow=</span>r)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>XX <span class="ot">&lt;-</span> <span class="fu">apply</span>(XX, <span class="dv">2</span>, dplyr<span class="sc">::</span>cummean)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="dv">1</span><span class="sc">:</span>n, XX[,<span class="dv">1</span>], <span class="at">type=</span><span class="st">"n"</span>, </span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab=</span><span class="st">"n"</span>, <span class="at">ylab=</span><span class="fu">expression</span>(<span class="fu">bar</span>(X[n])), </span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylim =</span> <span class="fu">range</span>(XX),</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>     <span class="at">main=</span><span class="st">"Law of Large Numbers - Cauchy Deviates"</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>r){</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    <span class="fu">lines</span>(<span class="dv">1</span><span class="sc">:</span>n, XX[,i], <span class="at">col=</span>scales<span class="sc">::</span><span class="fu">alpha</span>(<span class="st">"blue2"</span>,<span class="fl">0.15</span>))</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(<span class="dv">1</span><span class="sc">:</span>n, <span class="sc">+</span><span class="dv">1</span><span class="sc">/</span><span class="fu">sqrt</span>(<span class="dv">1</span><span class="sc">:</span>n), <span class="at">col=</span><span class="st">"red4"</span>, <span class="at">lty=</span><span class="dv">2</span>)</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(<span class="dv">1</span><span class="sc">:</span>n, <span class="sc">-</span><span class="dv">1</span><span class="sc">/</span><span class="fu">sqrt</span>(<span class="dv">1</span><span class="sc">:</span>n), <span class="at">col=</span><span class="st">"red4"</span>, <span class="at">lty=</span><span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="tails_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>There are occasional crazy large spikes which “blow up” the sample mean. These spikes are possible under any distribution with <span class="math inline">\(\mathbb{R}\)</span>-support, but they are common enough for the Cauchy that we ‘break’ the sample mean.</p>
<p>This is the essence of <em>heavy tails</em>. Heavy-tailed distributions are, in their purest form, distributions that are “so wild” they break the fundamental building blocks of statistics. Of course, not everything is doomed in the world of heavy tails - we can replace the sample mean with something like the sample median, which is more robust to these crazy spikes - but we have to tread much more carefully.</p>
<section id="whence-heavy-tails" class="level3">
<h3 class="anchored" data-anchor-id="whence-heavy-tails">Whence Heavy Tails?</h3>
<p>So where do heavy tails come from? We discuss one example below, stock market returns, where heavy tails are a fundamental feature of the underlying data generating process. In the land of theory, however, it’s a bit harder to see what might give rise to heavy tails. But whenever we want to ‘break’ something in math, we have one ever useful standby - division by zero.</p>
<p>Suppose we have two IID standard normal random variables <span class="math inline">\(X, Y\)</span>. What is the behavior of their ratio <span class="math inline">\(Z = X / Y\)</span>? It turns out that <span class="math inline">\(Z\)</span> has a Cauchy distribution! On some level, this makes sense: if <span class="math inline">\(Y\)</span> is standard normal, it is “usually near 0” in some probabilistic sense, so we are “usually nearly dividing by 0”. While exact division by zero isn’t something we need to worry about for continuous <span class="math inline">\(Y\)</span>, we are close to division by 0 whenever <span class="math inline">\(Y\)</span> is small. Diving deeper, we know that dividing by small numbers gives large numbers, so the probability <span class="math inline">\(\P(|Y| &lt; \epsilon)\)</span> for some small <span class="math inline">\(\epsilon\)</span> essentially gives us the probability of very large terms showing up, <span class="math inline">\(\P(1/|Y| &gt; 1/\epsilon)\)</span>.</p>
<p>Concisely, the Cauchy is “usually infinity” in the same way that the normal distribution is “usually zero”. Never exactly, but often enough for that to define its behavior.</p>
<p>Stepping back, this gives us point of caution in our actual statistical work. Be very cautious dividing by random variables, particularly if they have significant mass near zero. Even if you are lucky enough to never <em>exactly</em> divide by zero, you are <em>probabilisticly</em> close enough that things go horribly wrong. We will next discuss one circumstance in which this happens with some regularity.</p>
</section>
<section id="heavy-tails-in-finance" class="level3">
<h3 class="anchored" data-anchor-id="heavy-tails-in-finance">Heavy Tails in Finance</h3>
<p>Suppose that the price of a stock is given by <span class="math inline">\(S_t\)</span> - here <span class="math inline">\(S_t\)</span> is a <em>random function</em>, giving the price of the stock at any time <span class="math inline">\(t\)</span>. It is common to report the <em>returns</em> of investments in this stock over some time interval:</p>
<p><span class="math display">\[R_{t_1 \to t_2} = \frac{S_{t_2}}{S_{t_1}} - 1\]</span></p>
<p>In more mathematical contexts, we instead use <em>continuous</em> or <em>log</em> returns to measure performance:</p>
<p><span class="math display">\[R_{t_1 \to t_2} = \log\left(\frac{S_{t_2}}{S_{t_1}}\right)\]</span> For very small time intervals, <span class="math inline">\(S_{t_2} \approx S_{t_1}\)</span>, and these quantities converge. Log returns are preferred in mathematiacl contexts because they aggregate nicely over time and you don’t have to worry about compounding effects.<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a></p>
<p>Regardless of the return convention used, we see division by the random quantity <span class="math inline">\(S_{t_1}\)</span> and the hair on the back of our head should stand up. In more economic terms, <span class="math inline">\(S_t\)</span> can go to zero (or near zero) whenever a company enters bankruptcy. And that’s something that happens! Not every day to every company, but it’s certainly not unheard of.</p>
<p>The impact of bankruptcy is clearest in log-returns: when a stock goes to zero, it has a <span class="math inline">\(-\infty\%\)</span> return: there’s simply no coming back from that, no matter how long and how regularly the stock otherwise goes up. Because of that possibility, the law of large numbers does not apply to individual stock returns. Single-stock investing, no matter how long a time frame, remains risky: no LLN guarantees kick in.</p>
<p>While this is not a quantitative investing class, it’s also important to note that a multi-asset portfolio fails to have this issue unless there’s a real chance that every company in the portfolio goes backrupt at the same time. With multi-asset investment, we work instead with classical returns: even if half the portfolio goes to zero, the losses to the overall portfolio are capped at 50% and so things aren’t <em>too</em> heavy failed. (Note how this argument breaks down as you add leverage - your losses are no longer capped!)</p>
<p>Portfolio diversification is a very good thing!</p>
</section>
</section>
<section id="degrees-of-heaviness" class="level2">
<h2 class="anchored" data-anchor-id="degrees-of-heaviness">Degrees of Heaviness</h2>
<p>So far, we have discussed three distributions:</p>
<ul>
<li>Bernoulli (indicators)</li>
<li>Normal</li>
<li>Cauchy</li>
</ul>
<p>These exist on a spectrum of “heavy-tailedness”.</p>
<p>Bounded random variables, like the Bernoulli, have the thinnest and nicest possible tails. After the end of their support, the PDF goes to zero and the tails vanish into nothingness.</p>
<p>On the other end, distributions like the Cauchy are about as heavy tailed as possible. Recall that a PDF has to integrate to 1. The family of Cauchy-like PDFs</p>
<p><span class="math display">\[f_X(x) = \frac{a}{1+|x|^c} \]</span></p>
<p>only integrate to <span class="math inline">\(1\)</span> for <span class="math inline">\(c &gt; 1\)</span>. (Things blow up precisely at <span class="math inline">\(c = 1\)</span>.) So, while we could go a bit more tail-tastic than the Cauchy, taking <span class="math inline">\(c = 1.5\)</span> or even <span class="math inline">\(c = 1.1\)</span>, things really can’t get any heavier before we fail to have a PDF at all. Put another way, if a distribution has tails too much heavier than a Cauchy, it’s not only too wild to have a mean, it’s too wild to even define a real distribution!</p>
<p>But there is a wide range of distributions between these two families, broadly those with ‘exponential decay’ tails. We have already seen the normal distribution which has ‘squared exponential’ tails: <span class="math inline">\(f_X(x) \propto e^{-x^2}\)</span>. We can thicken these tails by removing the square, giving us the Laplace distribution $f_X(x) <span class="math inline">\(e^{-|x|}\)</span>. We can also ‘thin’ the normal tails further by increasing the exponent, giving even faster decay: <span class="math inline">\(f_X(x) \propto e^{-x^4}\)</span> or <span class="math inline">\(f_X(x) \propto e^{-x^6}\)</span>. These distributions, generally known as “generalized normal” or “Subbotin” distributions, are less common, but have been applied in a <a href="https://dx.doi.org/10.1214/22-AOAS1723">few settings successfully</a>.</p>
<p>“Heavy tails” are not a totally distinct field of models - they are just the hardest, nastiest limits of normal behaviors that require us to rethink many of our basic assumptions. In general, if we’re worried about large deviations having large effects, we need to change our estimator to minimize the impact of a large value. Classically, tools like the sample median are used, but we also have Windsorized means, robust regression methods, <em>etc</em>. In more modern work, particularly “CS-heritage” machine learning theory, 0/1-classification problems take center stage and we can sidestep any discussion of heavy tails by only looking at indicator functions (right/wrong prediction), which have bounded tails by construction.</p>
<p>Unfortunately, these tools often lack the nice properties we associated with means and expectations: notably, they do not play quite as well with differentiation. But that’s a story for another course.</p>
<p>As we progress in this course, we won’t see too much more about heavy-tailed random variables. Our goal is to learn all the probability necessary for classical (mean-oriented) statistics and machine learning. But it’s always good to keep the Cauchy in the back of your mind: it’s easy to worth with, but it still breaks nearly <em>everything</em>. Whenever we claim a new result or discuss a phenomenon, test it against a Cauchy - does it need an LLN to work? Is there a transformation we can apply to our random variable to guarantee an LLN holds? This will help sharpen your intuition for when the “standard” guarantees of probability hold and when they fail.</p>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Recall the “Random In, Random Out” principle.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Recall that “probability-mode” thinking is ‘forward’–given a distributional model, what predictions can I make about future observations–while “statistics-mode” thinking is ‘backwards’–given some observational data, what can I infer about the distribution of the underlying population? Statistical <em>theory</em> blends both of these since we need to think “forward” to figure out a procedure that will reliably work “backwards”, but statistical <em>practice</em> is looking from finite data to distribution.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Ultra-pedantically, this is actually a <em>series</em> of estimators, <span class="math inline">\(\textsf{MEAN}_1(X_1)\)</span>, <span class="math inline">\(\textsf{MEAN}_2(X_1, X_2)\)</span>, <span class="math inline">\(\dots\)</span>, but even I’m not sufficiently obnoxious to dwell on this point except in the vanishingly rare circumstances where it matters.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>This use of “bias” is not (necessarily) related to concepts like prejudice. It’s the older meaning that still comes up in idioms like “cut on a bias” as opposed to “cut straight” in carving meat. See <a href="https://en.wikipedia.org/wiki/Bias_of_an_estimator">Wikipedia</a> for more discussion.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>Later in this course, we will discuss the decomposition of error for <em>biased</em> estimators.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>The mathematically minded reader might note that we have not specified the type of convergence here. More precisely, what exactly does it mean for a random quantity <span class="math inline">\(\overline{X}_n\)</span> to converge to a non-random quantity like <span class="math inline">\(\mu_X\)</span>? From our discussion of <a href="./continuous_rvs.html">continuous random variables</a>, we know that <span class="math inline">\(\P(\overline{X}_n = \mu_X)\)</span> is either zero or ill-defined, so something more subtle is at play here. In fact, there are actual several different “modes of convergence” we could use here; each gives rise to a different Laws of Large Numbers. We will not worry about such distinctions in this course.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>Like so many things in the history of science, Markov’s Inequality is not named after its original discoverer, Pafnuty Chebyshev, but rather his student, Andrey Markov. See <a href="https://en.wikipedia.org/wiki/Stigler's_law_of_eponymy">Stigler’s Law of Eponomy</a> for an exhaustive discussion of this cruel phenomenon.<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>We can also estimate <span class="math inline">\(\sigma_X\)</span> from the data, but it makes the math just a bit more cumbersome, so we’ll hold off on it for now.<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>In general, <span class="math inline">\(\E[X^p] \text{ exists } \implies \E[X^q]
\text{ exists for } p &gt; q\)</span>)<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>Standard returns aggregate nicely “across assets”, so a portfolio that is 50% stock A and 50% stock B will have the average (standard) return of A and B. Log returns do not satisfy this property. Conversely, a stock that has a log return of 10% followed by another log return of -10% is unchanged; a stock that has a classical return of 10%, followed by a classical return of -10%, is still down 1%. Both conventions are useful - it just depends which direction you “aggregate” more frequently. Classically, mathematical finance is more concerned with modelling one or two assets over a lot of very short intervals, preferring log returns, but multi-asset portfolio modeling is increasingly important, particularly in risk management contexts, and is more suited for classical returns.<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/michael-weylandt\.com\/STA9715\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>