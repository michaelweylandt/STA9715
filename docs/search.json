[
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "STA 9715 - Additional Resources and Course Policies",
    "section": "",
    "text": "Readings and practice problems will be assigned from the following freely-available textbooks:\n\nIntroduction to Probability by Blitzstein and Hwang (BH)\nMathematics for Machine Learning by Diesenroth, Faisal, and Ong (DFO)\nIntroduction to Probability by Grinstead and Snell (GS)\n\nFor students wishing to dig deeper or to consult alternative resources, the following books are also recommended:\n\nStatistical Inference by Casella and Berger (Chapters 1-5)\nProbability with Applications in Engineering, Science, and Technology by Carlton and Devore (Chapters 1-4)\nFundamentals of Probability: A First Course by DasGupta (DG)1\n\n\n\nSTA9715 will use Piazza as the course discussion board. Students are encouraged to direct all questions about course topics or logistics to Piazza; use of a public anonymous discussion board allows students to benefit from the insights of their classmates and allows instructors to answer questions publicly to the benefit of all students.\nStudents are encouraged to use Piazza’s private question feature if they need to contact the instructor directly. Please only use private questions for personal inquiries: questions about the technical substance of the course can and should be asked (pseudonymously) in the public section of Piazza.\nPiazza login information will be distributed through CUNY Brightspace.\n\n\n\nStudents are encouraged to make use of any and all external resources, including generative AI tools, for at home study and for practice problems. Students are strongly encouraged to collaborate on weekly practice problems. You are ultimately responsible for the correctness of any submitted materials - “the AI told me so” is not a valid defense.\nNote that official solutions for many practice problems can be found online on the BH homepage.\nThe instructor will provide a formula sheet for use during in-class quizzes, tests, and final exam. No alternative resources may be used during these activities."
  },
  {
    "objectID": "resources.html#course-resources",
    "href": "resources.html#course-resources",
    "title": "STA 9715 - Additional Resources and Course Policies",
    "section": "",
    "text": "Readings and practice problems will be assigned from the following freely-available textbooks:\n\nIntroduction to Probability by Blitzstein and Hwang (BH)\nMathematics for Machine Learning by Diesenroth, Faisal, and Ong (DFO)\nIntroduction to Probability by Grinstead and Snell (GS)\n\nFor students wishing to dig deeper or to consult alternative resources, the following books are also recommended:\n\nStatistical Inference by Casella and Berger (Chapters 1-5)\nProbability with Applications in Engineering, Science, and Technology by Carlton and Devore (Chapters 1-4)\nFundamentals of Probability: A First Course by DasGupta (DG)1\n\n\n\nSTA9715 will use Piazza as the course discussion board. Students are encouraged to direct all questions about course topics or logistics to Piazza; use of a public anonymous discussion board allows students to benefit from the insights of their classmates and allows instructors to answer questions publicly to the benefit of all students.\nStudents are encouraged to use Piazza’s private question feature if they need to contact the instructor directly. Please only use private questions for personal inquiries: questions about the technical substance of the course can and should be asked (pseudonymously) in the public section of Piazza.\nPiazza login information will be distributed through CUNY Brightspace.\n\n\n\nStudents are encouraged to make use of any and all external resources, including generative AI tools, for at home study and for practice problems. Students are strongly encouraged to collaborate on weekly practice problems. You are ultimately responsible for the correctness of any submitted materials - “the AI told me so” is not a valid defense.\nNote that official solutions for many practice problems can be found online on the BH homepage.\nThe instructor will provide a formula sheet for use during in-class quizzes, tests, and final exam. No alternative resources may be used during these activities."
  },
  {
    "objectID": "resources.html#academic-integrity-policy",
    "href": "resources.html#academic-integrity-policy",
    "title": "STA 9715 - Additional Resources and Course Policies",
    "section": "Academic Integrity Policy",
    "text": "Academic Integrity Policy\nI fully support CUNY’s Policy on Academic Integrity, which states, in part:\n\nAcademic dishonesty is prohibited in The City University of New York. Penalties for academic dishonesty include academic sanctions, such as failing or otherwise reduced grades, and/or disciplinary sanctions, including suspension or expulsion.\n\n\nAcademic integrity is at the core of a college or university education. Faculty assign essays, exams, quizzes, projects, and so on both to extend the learning done in the classroom and as a means of assessing that learning. When students violate the academic integrity policy (i.e., “cheat”), they are committing an act of theft that can cause real harm to themselves and others including, but not limited to, their classmates, their faculty, and the caregivers who may be funding their education. Academic dishonesty confers an unfair advantage over others, which undermines educational equity and fairness. Students who cheat place their college’s accreditation and their own future prospects in jeopardy.\n\nIn this course, expectations for academic integrity are straightforward: no use of unauthorized materials on weekly quizzes, mid-semester tests, or the course final exam. Unless explicitly stated otherwise by the instructor in writing, the only authorized materials are the instructor-provided formula sheets.\nAcademic sanctions in this class will range from an F on the Assignment to an F in this Course. A report of suspected academic dishonesty will be sent to the Office of the Dean of Students.\nStudents are encouraged to contact the instructor with any questions or concerns related to matters of academic integrity."
  },
  {
    "objectID": "resources.html#course-accomodations",
    "href": "resources.html#course-accomodations",
    "title": "STA 9715 - Additional Resources and Course Policies",
    "section": "Course Accomodations",
    "text": "Course Accomodations\n\nDisability Services\nIt is CUNY policy to provide Accommodations and Academic Adjustments to students with disabilities.\nAny student who has a disability who may need accommodations in this class should register as early as possible with Student Disability Services (SDS). Your registration with Student Disability Services is confidential, and is not recorded on your Baruch Academic Record. SDS can be reached by email at disability.services@baruch.cuny.edu, by phone at 646-312-4590, or in person at NVC 2-272.\nPlease note that the instructor cannot provide accommodations unless requested by SDS.\n\n\nReligious Accomodations\nIt is CUNY policy to provide accommodations for students’ sincerely held religious beliefs. If a religious accommodation is requested, please contact the instructor at least two weeks in advance.\n\n\nUnexcused Abscence Policy\nAttendance is not required, but lecture recordings will not be provided. Students are responsible for the content of all sessions missed."
  },
  {
    "objectID": "resources.html#personal-resources2",
    "href": "resources.html#personal-resources2",
    "title": "STA 9715 - Additional Resources and Course Policies",
    "section": "Personal Resources2",
    "text": "Personal Resources2\nTake care of yourself. Do your best to maintain a healthy lifestyle this semester by eating well, exercising, avoiding drugs and alcohol, getting enough sleep and taking some time to relax. This will help you achieve your goals and cope with stress.\nAll of us benefit from support during times of struggle. You are not alone. Asking for support sooner rather than later is often helpful.\n\nMental Health Resources\nIf you or anyone you know experiences significant academic stress, difficult life events, or feelings like anxiety or depression, I strongly encourage you to seek support.\nThe Baruch Counselling Center is here to help. You can visit them in person at 137 E 25th St, 9th floor or call them at 646-312-2155 during normal business hours; you can make an appointment online here. For more immediate support, please call NYC WELL (1-888-NYC-WELL or 1-888-692-9355).3\nAsking for help is often difficult: consider reaching out to a friend, family, or a member of the faculty you trust for help getting connected to support that can help.\nIf you are worried about a friend or classmate, consider reaching out to the Baruch Campus Intervention Team.\n\n\nPhysical Health\nHealthy CUNY promotes well-being and a culture of health in order to foster the academic and life success of all CUNY students. They can connect you with a variety of campus- and community-based healthcare providers.\nBaruch Health Services provides students with a full range of clinical health services. Call 646-312-2040 or email StudentHealthCareCenter@baruch.cuny.edu to make an appointment.\n\n\nFood Security\nAll CUNY students have access to CUNY Food Pantries located throughout the five boroughs, thanks to the CUNY CARES program. CUNY CARES is also able to help qualifying students with SNAP (“Food Stamps”) enrollment.\n\n\nImmigration Status\nCUNY Citizenship Now! provides confidential, high-quality immigration law services to all CUNY students.\nNote that Citizenship Now!’s primary Manhattan office is located in the Heights, not on the Baruch campus and that an appointment is strongly recommended. Call 646-664-9350 during business hours for more information."
  },
  {
    "objectID": "resources.html#footnotes",
    "href": "resources.html#footnotes",
    "title": "STA 9715 - Additional Resources and Course Policies",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFree to CUNY Students. Please contract instructor if having access issues↩︎\nLanguage adapted from Professor Ryan Tibshirani (UC Berkeley).↩︎\nDescriptions of Baruch and CUNY resources adapted from program websites.↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STA 9715 - Applied Probability",
    "section": "",
    "text": "Welcome to the course website for STA 9715 (Fall 2024)!\nSTA 9715 is an Applied Probability course targeted at students in the MS in Business Analytics, MS in Statistics, and MS in Quantitative Methods programs.\nThis site hosts the unofficial Course Syllabus, Course Policies, and Course Learning Objectives. Official copies of these documents can be found on CUNY Blackboard. Additional course handouts can also be found on this site.\nInstructor: Michael Weylandt"
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "STA 9715 - Notes",
    "section": "",
    "text": "Supplemental course notes will be posted here.\n\nWeek 1: Foundations of Probability\n\nQuanta Magazine: “Perplexing the Web, One Probability Puzzle at a Time”\nWIRED Magazine: “Statistician Answers Stats Questions From Twitter | Tech Support”\n\nMore Jeff Rosenthal\n\nHarvard STAT 110 Playlist\n\nThis is the course on which our textbook (BH) is based\n\n\n\n\nWeek 2: Conditionality and Marginality\n\nThe YouTube channel 3blue1brown makes excellent ‘explainers’ about a variety of mathematical topics. Check out his videos on Bayes’ rule:\n\nThe Geometry of Changing Beliefs\n\nSee also A Quick Proof of Bayes Theorem\n\nThe Medical Test Paradox\n\nConditioning: The Soul of Statistics\nNotes on Expectations\n\n\n\nWeek 3: Discrete Probability\n\n\nWeek 4: From Discrete to Continuous Random Variables\n\n\nWeek 5: Heavy Tails\n\n\nWeek 6: Random Vectors\n\n\nWeek 7: Covariance and Correlation\n\n\nWeek 8: The Multivariate Normal Distribution and its Progeny\n\n\nWeek 9: Probability, Polling, and Prediction\n\n\nWeek 10: Perils and Paradoxes in Expectations\n\n\nWeek 11: Probability Inequalities and Limit Theorems\n\n\nWeek 12: Distributional Limits and the Central Limit Theorem\n\n\nWeek 13: Concentration of Measure: Generalized Limit Theory\n\n\nWeek 14: Computing with Randomness"
  },
  {
    "objectID": "objectives.html",
    "href": "objectives.html",
    "title": "STA 9715 - Course Learning Objectives",
    "section": "",
    "text": "This course provides a comprehensive introduction to applied probability and probability distributions. Students will learn probability with an understanding of its applications in statistical inference. Topics include discrete and continuous random variables and distributions, such as the binomial, negative binomial, Poisson, geometric, uniform, normal, exponential, gamma (\\(\\Gamma\\)), beta (\\(B\\)), chi-square (\\(\\chi^2\\)), \\(t\\), and \\(F\\) distributions. This course thoroughly develops topics as transformation of variables, joint distributions, bivariate normal, expectations, conditional distributions and expectations, moment-generating functions, distribution of sums of random variables, means and variances of sums, ratios of independent variables, and central limit theorem. Students will acquire an excellent background to proceed to statistical inference."
  },
  {
    "objectID": "objectives.html#official-course-description",
    "href": "objectives.html#official-course-description",
    "title": "STA 9715 - Course Learning Objectives",
    "section": "",
    "text": "This course provides a comprehensive introduction to applied probability and probability distributions. Students will learn probability with an understanding of its applications in statistical inference. Topics include discrete and continuous random variables and distributions, such as the binomial, negative binomial, Poisson, geometric, uniform, normal, exponential, gamma (\\(\\Gamma\\)), beta (\\(B\\)), chi-square (\\(\\chi^2\\)), \\(t\\), and \\(F\\) distributions. This course thoroughly develops topics as transformation of variables, joint distributions, bivariate normal, expectations, conditional distributions and expectations, moment-generating functions, distribution of sums of random variables, means and variances of sums, ratios of independent variables, and central limit theorem. Students will acquire an excellent background to proceed to statistical inference."
  },
  {
    "objectID": "objectives.html#course-learning-objectives",
    "href": "objectives.html#course-learning-objectives",
    "title": "STA 9715 - Course Learning Objectives",
    "section": "Course Learning Objectives",
    "text": "Course Learning Objectives\nStudents successfully completing STA 9715 will:\n\nDefine and manipulate probability mass and density, expectation, and variance for discrete and continuous random variables.\nDefine and manipulate conditional probability, expectation, and variance.\nDefine and manipulate important named probability distributions including, but not limited to, the normal distribution, uniform distribution, and \\(t\\)-distribution.\nManipulate collections of random variables, characterizing them in terms of covariance and correlation, and establishing properties of their limits and sums.\nDefine and manipulate concentration of measure phenomena including the central limit theorem.\n\nAdditionally, STA 9715 will review important mathematical concepts used elsewhere in the statistics curriculum.\nThe following course elements contribute to these goals:\n\nContribution of Course Elements to Learning Goals\n\n\n\n\n\n\n\n\n\n\nPractice Problems\nLearning Goal 1\nLearning Goal 2\nLearning Goal 3\nLearning Goal 4\nLearning Goal 5\n\n\n\n\nWeek 1\n✓\n\n\n\n\n\n\nWeek 2\n✓\n✓\n\n\n\n\n\nWeek 3\n✓\n\n✓\n\n\n\n\nWeek 4\n✓\n\n✓\n\n\n\n\nWeek 5\n✓\n✓\n✓\n\n\n\n\nWeek 6\n✓\n✓\n✓\n✓\n\n\n\nWeek 7\n✓\n✓\n✓\n✓\n\n\n\nWeek 8\n✓\n✓\n✓\n✓\n\n\n\nWeek 10\n✓\n✓\n✓\n✓\n✓\n\n\nWeek 11\n✓\n✓\n✓\n✓\n✓\n\n\nWeek 12\n✓\n✓\n✓\n✓\n✓\n\n\nWeek 13\n✓\n✓\n✓\n✓\n✓"
  },
  {
    "objectID": "objectives.html#program-learning-goals",
    "href": "objectives.html#program-learning-goals",
    "title": "STA 9715 - Course Learning Objectives",
    "section": "Program Learning Goals",
    "text": "Program Learning Goals\nThis course contributes to the program learning goals of several MS programs offered by the Zicklin School of Business.\n\nMS in Business Analytics\nThis course contributes to the following Program Learning Goals for the MS in Business Analytics:\n\nMSBA Program Learning Goals\n\n\n\n\n\n\n\nSTA 9715 Learning Goal\nMSBA Learning Goal\nDescription\n\n\n\n\n\nData Management\nStudents will be able to apply methods, tools, and software for acquiring, managing/storing, and accessing structured and unstructured data. Students will also demonstrate knowledge of the strategic uses of data.\n\n\n✓\nFoundational Statistical / Quantitative Skills\nStudents will be able to prepare data for statistical analysis, perform basic exploratory and descriptive analysis as well as employ foundational statistical techniques needed to analyze data.\n\n\n✓\nAdvanced Statistical/Quantitative Skills\nStudents will be able to build and interpret advanced predictive models. Students will be able to combine business rules and mathematical models to optimize business decisions from data.\n\n\n✓\nEthical Awareness\nStudents will be able to articulate an understanding of ethical issues in all phases of business analytics with particular emphasis on the new possibilities afforded by the emergence of big data.\n\n\n✓\nProfessional Communication\nStudents will be able to explain complex analytical models and their results orally and in writing to technical and non technical/lay audiences.\n\n\n\nKnowledge Integration\nStudents will be able to apply the three key types of analytics (descriptive, predictive, and prescriptive) in a business domain to add value to business decision-making.\n\n\n\n\n\nMS in Quantitative Methods & Modeling\nThis course contributes to the following Program Learning Goals for the MS in Quantitative Methods & Modeling:\n\nMSQMM Program Learning Goals\n\n\n\n\n\n\n\nSTA 9715 Learning Goal\nMSQMM Learning Goal\nDescription\n\n\n\n\n✓\nOperations Research & Mathematical Modeling\nStudents will be able to effectively model, evaluate, and solve quantitative (business) problems using quantitative modeling methods (e.g. deterministic and probabilistic operations research techniques).\n\n\n✓\nStatistics\nStudents will be able to correctly apply appropriate statistical methods when defining, solving, and analyzing problems.\n\n\n\nTechnology Competency\nStudents will be able to use current technological tools, including spreadsheets and specialized software, when solving problems.\n\n\n✓\nProfessional Communication\nStudents will be able to effectively communicate their problem solving methods and solutions to technical and non-technical audiences.\n\n\n\n\n\nMS in Statistics\nThis course contributes to the following Program Learning Goals for the MS in Statistics:\n\nMS Statistics Program Learning Goals\n\n\n\n\n\n\n\nSTA 9715 Learning Goal\nMS Stat Learning Goal\nDescription\n\n\n\n\n✓\nGeneral Statistical Competence\nStudents will be able to apply appropriate probability models and statistical techniques when analyzing problems frm business and other fields.\n\n\n✓\nStatistical Practice\nStudents will become familiar with the standard tools of statistical practice for multiple regression, along with the tools of a subset of specialized statistical areas such as multivariate analysis, applied sampling, time series analysis, experimental design, data mining, categorical analysis, and/or stochastic processes.\n\n\n\nTechnology Competency\nStudents will learn to use one or more of the benchmark statistical software platforms, such as SAS or R."
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "STA 9715 - Course Syllabus",
    "section": "",
    "text": "Professor Michael Weylandt\nDepartment of Information Systems & Statistics\nZicklin School of Business\nBaruch College, CUNY"
  },
  {
    "objectID": "syllabus.html#instructor",
    "href": "syllabus.html#instructor",
    "title": "STA 9715 - Course Syllabus",
    "section": "",
    "text": "Professor Michael Weylandt\nDepartment of Information Systems & Statistics\nZicklin School of Business\nBaruch College, CUNY"
  },
  {
    "objectID": "syllabus.html#course-meetings",
    "href": "syllabus.html#course-meetings",
    "title": "STA 9715 - Course Syllabus",
    "section": "Course Meetings",
    "text": "Course Meetings\n\nLectures\n\nMondays 6:05pm-9:00pm (In-Person)\n\nBaruch Main Campus (1 Bernard Baruch Way)\nNewman Vertical Campus (NVC) 9-170\n\n\n\n\nOffice Hours\n\nIn-Person\n\nBaruch Main Campus (1 Bernard Baruch Way)\nNewman Vertical Campus (NVC) 11-246\nMondays 4:30-5:30pm\nSubject to periodic cancellation (communicated via Brightspace)\n\nVirtual:\n\nThursdays 4:30pm-5:30pm\nZoom link provided via Brightspace"
  },
  {
    "objectID": "syllabus.html#grading",
    "href": "syllabus.html#grading",
    "title": "STA 9715 - Course Syllabus",
    "section": "Grading",
    "text": "Grading\n\n50% Mid-Semester Tests (Best two of three; 100 points each; 200 points total)\n\nOctober 7th, covering Weeks 1-4\nNovember 11th, covering Weeks 5-8\nDecember 9th, covering Weeks 10-13\n\n50% Comprehensive Final Exam (200 points total)\n\nScheduled by Baruch Registrar (tentatively December 16th at 6:00pm)\n\n\nSmall amounts of extra credit will be given for active and helpful participation in the course discussion board (Piazza) at the instructor’s discretion.\nFinal course grades will be curved in accordance with relevant program, departmental, school, and college policies.1\n\nWeekly Quizzes\nIn lieu of homework, I will provide a short list of practice problems following each lecture. At the start of the following lecture, a short (15 minute) quiz drawn closely from the practice problems will be administered. The quiz questions will not be verbatim from the practice problems, but if you can answer the practice problems quickly and fluently, the quiz should pose little difficulty.\nThese in-class weekly quizzes will generate extra credit applied to your final aggregate score. Each quiz will receive a score out of 3 added directly to your final score. Because the final aggregate score is out of 400, perfect scores on all 12 weekly quizzes can raise your final aggregate score (pre-curve) up to 9%.\nTo take part in the weekly quizzes, please come to class with both i) a black or blue pen; and ii) a red pen (for peer grading) each week.\nMake-up opportunities for the weekly quizzes will only be allowed in exceptional and unforeseeable circumstances.\n\n\nRegrading Policy\nIf you feel an assignment has been improperly graded, please contact the instructor by private message on the course discussion board within 48 hours of the graded assignment being returned. Note that the instructor will regrade the assignment de novo, so your grade may be adjusted upwards or downwards."
  },
  {
    "objectID": "syllabus.html#tentative-course-schedule",
    "href": "syllabus.html#tentative-course-schedule",
    "title": "STA 9715 - Course Syllabus",
    "section": "Tentative Course Schedule",
    "text": "Tentative Course Schedule\n\n\n\n\n\n\n\n\n\n\n\n\nWeek\nLecture Date\nTopics\nPre-Reading\nPost-Reading\nPractice Problems\nMid-Semester Tests\n\n\n\n\n1\nSeptember 9th, 2024\nFoundations of Probability:\n\nCourse Overview\nThe Logic of Theory\nProbability Axioms\nElements of Combinatorics\n\nDFO §6.1,\nGS §1.2, §3.1-3.3\nBH §1.1-1.4, §1.6-1.7, §A.1-A.2\nBH §1.9: 3, 4, 5, 12, 15, 23, 24, 26, 28, 29, 31, 41, 44, 45, 49\n\n\n\n2\nSeptember 16th, 2024\nConditionality and Marginality:\n\nConditional Probabilities\nExpectation\nVariance\nMoments\nConditional Expectation\n\nDFO §6.3\nGS §4.1, 6.1-6.2\nBH §2.1-2.9, §4.1-4.6, §A.8-A.10\nBH §2.11: 1, 3, 9, 11, 14, 19, 30, 48, 59\n§4.12: 2, 6, 12, 14, 34, 35\n\n\n\n3\nSeptember 23rd, 2024\nDiscrete Probability Calculations:\n\nProbability Mass Functions\nNamed Discrete Distributions\n\nGS §5.1\nBH §3.1-3.10\nBH: §3.12: 2, 3, 6, 9, 10, 15, 17, 18, 21, 23, 24, 25, 31, 34, 35, 38, 40\n\n\n\n4\nSeptember 30th, 2024\nFrom Discrete to Continuous Random Variables:\n\nCalculus Review\nProbability Density Functions\nCumulative Distribution Functions\n\nDFO §6.2, 6.5\nGS §5.2, 4.2, 6.3\nBH §5.1-5.8\nBH §5.10: 1, 3, 4, 6, 8, 9, 18, 19, 21, 24, 25, 27, 34, 37, 42, 44\n\n\n\n5\nOctober 7th, 2024\nHeavy Tails: What and Why?\nNone (test prep)\nBH §6.1-6.3\nHandout to be distributed\nBH §6.10: 3, 9, 11, 12\nTest I: Covering Weeks 1-4\n\n\n6\nTUESDAY October 15th, 2024 (Note date change)\nRandom Vectors:\n\nReview of Multivariable Calculus\nJoint Distributions\nMarginal Distributions\nConditional Distributions\n\nDFO §5.1-5.2\nBH §7.1-7.2,§ A.3, §A.6-A.7\nBH §7.8: 1, 4, 5, 6, 8, 9, 10, 11, 13, 14, 16, 17, 18,\n\n\n\n7\nOctober 21st, 2024\nCovariance and Correlation: Working with Linear Combinations of Random Variables\nGS §7.1-7.2\nBH §6.4-6.6, §7.3\nBH §7.8: 31, 33, 36, 37, 38, 41, 43, 48, 49, 51, 54, 59\n\n\n\n8\nOctober 28th, 2024\nThe Multivariate Normal Distribution and its Progeny:\n\nReview of Linear Algebra\nProperties of the Multivariate Normal Distribution\nDerived Distributions\n\nDFO §2.1-2.7, §3.1-3.4, $4.2-4.4\nBH §7.5-7.6, §10.4\nBH 7.8: §72, 73, 74, 77, 78,\nHandout to be provided\n\n\n\n9\nNovember 4th, 2024\nSpecial Topics: Probability, Polling, and Prediction\n\n\n\n\n\n\n10\nNovember 11th, 2024\nPerils and Paradoxes in Expectations:\n\nSelection and Sampling Biases\nImplications for (Over-)Fitting of Models\n\nNone (test prep)\nDFO §8.1-8.3, §8.6\nBH §9.1-9.3, §9.5-9.7\nBH §4.12: 17\n§9.9: 1, 13, 15, 16, 25, 29, 38, 39, 40, 43,\nTest II: Covering Weeks 5-8\n\n\n11\nNovember 18th, 2024\nProbability Inequalities and Limit Theorems\nGS §8.1-8.2\nBH §10.1-10.2\nBH §10.7:\n1, 2, 4, 6, 7, 13, 15, 16, 21,\n\n\n\n12\nNovember 25th, 2024\nDistributional Limits and the Central Limit Theorem\nGS §9.1-9.3\nBH §10.3\nBH §10.7: 22, 23, 24, 26, 28, 29, 30, 36, 37\n\n\n\n13\nDecember 2nd, 2024\nConcentration of Measure: Generalized Limit Theory\nHandout to be distributed.\nHandout to be distributed.\nHandout to be distributed.\n\n\n\n14\nDecember 9th, 2024\nComputing with Randomness: an Introduction to Monte Carlo Methods\nNone (test prep)\n\nNone (last day of class)\nTest III: Covering Weeks 10-13\n\n\n\nNote on Week 9: On November 4th (Election Eve), we will meet at our regular time (6:05pm). Instead of our usual format (quiz, peer evaluation, review, new material), we will discuss a set of election-related special topics, including i) construction and evaluation of probabilistic election forecasts; ii) martingale properties and their implications for forecasting; iii) conformal calibration as applied to real-time election results. New Material from Week 8 (Multivariate Normal Distribution) will be quizzed during Week 10 (November 11th).\nAll syllabus provisions subject to change with suitable advance notice.\nChanges will be announced in class and via Brightspace."
  },
  {
    "objectID": "syllabus.html#pre--and-post-reading-suggestions",
    "href": "syllabus.html#pre--and-post-reading-suggestions",
    "title": "STA 9715 - Course Syllabus",
    "section": "Pre- and Post-Reading Suggestions",
    "text": "Pre- and Post-Reading Suggestions\nStudents learn material most effectively when exposed to it on multiple occasions, ideally using alternative presentations strategies and formats.2 To this end, suggested pre-reading and post-reading is provided for each week of the course. Students are encouraged to pre-read the recommended text, which typically presents that week’s material in a less technical / more intuitive manner, before each week’s course session. Similarly, students are encouraged to review the post-reading for each week after lecture to see additional examples of topics covered.\nWhile lectures will focus primarily on ‘big picture’ and ‘major themes’, the recommended reading, especially the post-reading from BH, provides additional coverage of relevant technical detail.\nStudents with prior exposure to topics in probability may choose to omit pre-reading. In general, GS pre-reading introduces fundamentals of probability while DFO pre-reading reviews relevant mathematical tools. Students may also elect to consume post-reading as part of completing that week’s practice problems, rather than as a separate activity.\nStudents are responsible for all material appearing in the pre-reading, in lecture, and in the post-reading, but students will not be evaluated on reading per se."
  },
  {
    "objectID": "syllabus.html#footnotes",
    "href": "syllabus.html#footnotes",
    "title": "STA 9715 - Course Syllabus",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTheoretically, this may result in scores equivalent to an A in an un-curved course receiving a lower grade in this course. In practice, the instructor will design course assessments to induce a range of scores and does not anticipate “down-curving” happening.↩︎\nHaoyu Chen and Jiongjiong Yang. “Multiple Exposures Enhance Both Item Memory and Contextual Memory over Time”. Frontiers in Psychology 11. November 2020. DOI:10.3389/fpsyg.2020.565169↩︎"
  },
  {
    "objectID": "notes/expectations.html",
    "href": "notes/expectations.html",
    "title": "What to Expect when You’re Expecting: Notes on Expectations, Variances, and Probabiblities",
    "section": "",
    "text": "\\[\\newcommand{\\P}{\\mathbb{P}} \\newcommand{\\E}{\\mathbb{E}} \\newcommand{\\V}{\\mathbb{V}}\\]\nIn this note, I outline some of the basic properties of expectations and variances of random variables. I also show how many of these properties can be extended to probabilities via the use of indicator functions."
  },
  {
    "objectID": "notes/expectations.html#random-variables",
    "href": "notes/expectations.html#random-variables",
    "title": "What to Expect when You’re Expecting: Notes on Expectations, Variances, and Probabiblities",
    "section": "Random Variables",
    "text": "Random Variables\nSo far, we have mainly focused on probabilities of individual events (e.g., the probability of rolling a certain sum from a set of dice). This is useful enough, but somewhat limiting. If we have to start from scratch and build probabilities for every possible event from the raw sample space, we will never get anything done. A far more productive approach is to work with random variables. Formally, a random variable is a function from the sample space to the set of real numbers. That is, for every \\(\\omega \\in \\Omega\\), we can define a random variable \\(X\\) as \\(X(\\omega) = f(\\omega)\\). \\(X\\) inherits a probability measure from \\(\\Omega\\): \\[\\P(X = a) = \\P(X(\\omega) = a) = \\P(\\{\\omega: X(\\omega) = a\\}).\\] That is, we compute the probability of \\(X\\) taking a value \\(a\\) by looking at the probability of all inputs that could have lead to \\(a\\). (This induced measure is sometimes called the ‘push-forward’ because we get probabilities on \\(X\\) by pushing ‘raw’ probabilities on \\(\\Omega\\) into \\(X\\)-world.) Because \\(X\\) ever only takes one value at at time, the events \\(\\{X = a_1\\}, \\{X = a_2\\}, \\dots\\) are disjoint, which makes it much easier to calculate with \\(X\\) than it is with \\(\\omega\\). For example, \\[\\P(X = a \\text{ or } X = b) = \\P(X = a) + \\P(X = b) - \\P(X = a \\text{ and } X = b) = \\P(X = a) + \\P(X = b)\\]\nThis formal definition is great, but we can also just ‘start’ with \\(X\\) and avoid talking about the sample space \\(\\Omega\\) at all. Specifically, let’s define a random variable \\(X\\) by a set of probabilities and real values, subject to the constraint that the probabilities add to 1. For example, we may define the random variable \\(X\\) by:\n\nRandom Variable \\(X\\)\n\n\n\\(a\\)\n\\(\\P(X = a)\\)\n\n\n\n\n1\n\\(\\frac{1}{16}\\)\n\n\n2\n\\(\\frac{4}{16} = \\frac{1}{4}\\)\n\n\n3\n\\(\\frac{6}{16} = \\frac{3}{8}\\)\n\n\n4\n\\(\\frac{4}{16} = \\frac{1}{4}\\)\n\n\n5\n\\(\\frac{1}{16}\\)\n\n\n\nExamining this, we see that the sum \\(\\sum_{a=1}^5 \\P(X = a) = 1\\) so \\(X\\) here is a valid random variable.\n\n\n\n\n\n\nRandom Variables - Working Definition\n\n\n\nA random variable \\(X\\) is defined by a set of outcome-probability pairs \\(X \\equiv \\{(a_1, p_1), (a_2, p_2), \\dots\\}\\) such that:\n\nThe outcomes are all distinct \\(a_i \\neq a_j\\) for \\(i \\neq j\\);\nThe probabilities are non-negative: \\(p_i \\geq 0\\) for all \\(i\\); and\nThe probabilities sum to \\(1\\): \\(\\sum_i p_i = 1\\).\n\nThe probability expression \\(\\P(X = a)\\) is a ‘look-up’ expression:\n\\[\\P(X = a) = p \\text{ if and only if } (a, p) \\in X\\]\n\n\nComparing this to the “naive” definition of probability (all outcomes equal - events comprised of different numbers of outcomes), you might argue that we haven’t actually simplified anything: we’ve just moved the complexity around. Technically, you may be right, but I would argue that working from random variables as our starting point is hugely helpful. We have replaced the tedious and somewhat painful algebra of counting with the simple and easily automated algebra of summing. For instance, if we want to compute the probability that the random variable \\(X\\) we defined above is even, we only need to sum \\[\\P(X = 2) + \\P(X = 4) = \\frac{1}{4} + \\frac{1}{4} = \\frac{1}{2}.\\] No intersections, unions, permutations, or combinations to be seen.\n\n\n\n\n\n\nA Note on Notation\n\n\n\nIn this course, we will adopt a nearly universal notational convention: capital letters, e.g. \\(X\\), refer to random variables while \\(x\\) refers to fixed deterministic (non-random) quantities. In this convention, a statement like \\(\\P(X = x)\\) is asking what is the probability that a random quantity (\\(X\\)) takes a certain fixed value. When we have multiple random variables in play (\\(X, Y\\)), we can compute \\(\\P(X = Y)\\), the chance of a ‘tie’, but statements like \\(\\P(x = y)\\) are essentially meaningless since there’s no randomness.\n\n\nWhen the set of outcomes is ‘not too infinite’, we refer to \\(X\\) is as a discrete random variable and require the probabilities to sum to one. If \\(X\\) is ‘quite infinite’, we refer to \\(X\\) as a continuous random variable and require probabilities to integrate to one instead.1 Specifically, if \\(X\\) has:\n\na finite set of outcomes; or\nan infinite set of outcomes that can be listed (‘countably infinite’)\n\nwe’ll treat it as discrete. In this course, the only ‘countably infinite’ sets we deal with are the integers or subsets thereof (e.g. positive integers), so the heuristic of “discrete variables have integer outcomes or only finite numbers of outcomes” will serve you well."
  },
  {
    "objectID": "notes/expectations.html#expectations",
    "href": "notes/expectations.html#expectations",
    "title": "What to Expect when You’re Expecting: Notes on Expectations, Variances, and Probabiblities",
    "section": "Expectations",
    "text": "Expectations\nWhen faced with a random variable \\(X\\), we may ask (or be asked) “What’s going to Happen?” Clearly, because \\(X\\) is random, we can’t really be certain about what’s going to happen, but we can still make a ‘best guess’.\n\nLoss Functions and Best Predictions\nOf course, any notion of ‘best’ is context dependent: in simple problems, we may only care about ‘right / wrong’ predictions, but in many circumstances, the degree (and direction) of error in our prediction matters. If we are off by 1 degree in a weather prediction, that’s not bad, but if we are over by $1 on The Price is Right, we instantly lose.\nIn statistics, we often work with a loss function, a quantitative measure of the ‘pain’ we experience when our guess is wrong.2 Some examples of loss functions include:\n\nRight-or-Bust: \\[\\ell(\\text{guess}, \\text{truth}) = \\begin{cases} 0 & \\text{ if guess $=$ truth} \\\\ 1,000,000 & \\text{ if guess $\\neq$ truth}\\end{cases}\\]\nSquared Error: \\[\\ell(\\text{guess}, \\text{truth}) = (\\text{guess} - \\text{truth})^2\\]\nAbsolute Error: \\[\\ell(\\text{guess}, \\text{truth}) = |\\text{guess} - \\text{truth}|\\]\nClosest without Going Over: \\[\\ell(\\text{guess}, \\text{truth}) = \\begin{cases} \\text{truth} - \\text{guess} & \\text{ if guess $\\leq$ truth} \\\\ 1,000,000 & \\text{ if guess $&gt;$ truth} \\end{cases}\\]\n\n(Note that the number \\(1,000,000\\) here is a stand-in for any large number. It’s not special.)\nIn general, we allow any loss function satisfying \\(\\ell(x, y) \\geq 0\\) with equality if and only if \\(x = y\\); that is, we want our loss functions to be non-negative for any \\((x, y)\\) and zero only when we get exactly the right answer.\nOnce we commit to a loss function, the ‘best’ prediction is not too hard to figure out:\n\nRight-or-Bust gives us the mode, i.e., the single most probable value of \\(X\\)\nAbsolute Error gives us the median\nSquared Error gives us the mean\n\nGiven the ubiquity of means in statistics, you might expect squared error to be the ‘one true loss function’. In practice, very few loss functions are truly quadratic. But the general phenomenon of:\n\nsymmetric; and\nincreasing rate (the incremental pain of being off by 3 vs off by 2 is more than the incremental pain of off by 2 instead of off by 1)\n\nis quite common and squared error is the simplest mathematical model with those two properties. Accordingly, the mean is not the single best possible prediction for all scenarios, but it’s a very close to optimal prediction for a wide range of scenarios. Combine that with mathematical convenience and its no surprise that squared error loss and means dominate the field of statistics.\n\n\nExpected Values\nOk - that’s enough chit chat. Let’s just say I want my best possible prediction under a squared error loss. How do I actually compute it?\nLet us define the expectation of a random variable as follows:\n\n\n\n\n\n\nExpected Value of a Random Variable\n\n\n\nGiven a random variable \\(X\\), its expected value is defined as a probability-weighted average of all outcomes: that is,\n\\[ \\E[X] = \\sum_a \\P(X = a) * a \\]\nThe expression \\(\\E[X]\\) is pronounced “the expectation of \\(X\\)”.\n\n\nUsing our running example, we get\n\\[\\E[X] = 1 * \\frac{1}{16} + 2 * \\frac{4}{16} + 3 * \\frac{6}{16} + 4 * \\frac{4}{16} + 5 * \\frac{1}{16} = 3\\]\nNote here that the expectation is the center value of \\(X\\). That isn’t always the case, but it is when \\(X\\) is a symmetric random variable and here \\(X\\) is symmetric around \\(3\\).\nThere’s nothing special about \\(X\\) in expectations: it’s very possible to compute expectations of arbitrary random variables, including functions of \\(X\\) itself. For example, let’s try \\(\\E[(X - 3)^2 + 5]\\).\nBeing careful, let’s define a new random variable \\(Y = (X-3)^2 + 5\\). It’s not hard to check that \\(Y\\) is defined by\n\nDistribution of \\(Y\\)\n\n\ny\n\\(\\P(Y = y)\\)\n\n\n\n\n5\n\\(\\frac{1}{8}\\)\n\n\n6\n\\(\\frac{1}{2}\\)\n\n\n9\n\\(\\frac{3}{8}\\)\n\n\n\nThis gives us:\n\\[\\E[Y] = 5 * \\frac{1}{8} + 6 * \\frac{1}{2} + 9 * \\frac{3}{8} = 7\\]\nThat works, but it’s perhaps a little too much work. It turns out that we can just compute \\(\\E[(X - 3)^2 + 5]\\) directly without mentioning \\(Y\\).\n\n\n\n\n\n\nLaw of the Unconscious Statistician\n\n\n\nLaw of the Unconscious Statistician: expectations of functions of random variables can be computed by ‘naive’ substitution into the definition of expectation. That is,\n\\[\\E[f(X)] = \\sum f(x) \\P(X = x)\\]\n\n\nHere,\n\\[\\begin{align*}\n\\E[f(X)] &= f(1) * \\frac{1}{16} + f(2) * \\frac{4}{16} + f(3) * \\frac{6}{16} + f(4) * \\frac{4}{16} + f(5) * \\frac{1}{16} \\\\\n&= 9 * \\frac{1}{16} + 6 * \\frac{4}{16} + 5 * \\frac{6}{16} + 6 * \\frac{4}{16} + 9 * \\frac{1}{16} \\\\\n&= 7\n\\end{align*}\\]\n\n\nProperties of Expected Values\nExpected values satisfy many useful properties. From the “weighted sum” definition, it’s not hard to show:\n\nLinearity: \\[\\E[aX + b] = a\\E[X] + b\\]\nExpectation of a Constant: \\[\\E[c] = c\\]\nSums of Functions: \\[\\E[f(X) + g(X)] = \\E[f(X)] + \\E[g(X)]\\]\nRough Bounds: \\[\\min\\{X\\} \\leq \\E[X] \\leq \\max\\{X\\}\\]\nInterchange with Derivatives: \\[\\E\\left[\\frac{df}{dx}(X)\\right] = \\frac{d}{dx}\\E[f(X)]\\]\n\nWe’ll note more interesting properties of expectations in the following sections.\n\n\nIndicators and Expectations\nA particularly important function class we might consider are indicator functions. For any event \\(A\\), define \\(1_{A}(X)\\) as the function taking \\(1\\) if outcome \\(X\\) is in event \\(A\\) and \\(0\\) otherwise. For example, if \\(A\\) is the set of even numbers, \\(1_A(3) = 0\\) while \\(1_A(2) = 1\\). There is a deep connection between probabilities and expectations of indicator functions.\n\\[\\begin{align*}\n\\E[1_A(X)] &= 0 * \\P(X \\notin A) + 1 * \\P(X \\in A) \\\\\n&= \\P(X \\in A) \\\\\n&= \\P(A)\n\\end{align*}\\]\nBecause of this, any results we have about expectations can be used to derive similar results for probabilities. In particular, we know from statistical theory that sample means converge to expectations: this, in turn, implies that sample probabilities converge to true probabilities. We’ll discuss this below after we talk about variances.\n\n\nSums of Expectations\nSuppose we have two random variables \\((X, Y)\\). How can we compute the expectation of their sum? (By linearity above, if we can say something about the sum, we’ll be able to make similar claims about arbitrary linear combinations.)\nIn brief, the answer is that\n\\[\\E[X + Y] =\\E[X] + \\E[Y]\\]\nwithout assuming independence.\nNow let’s prove that: Let \\(Z = (X, Y)\\) be the compound random variable. Define \\(f_X(Z) = X\\) and \\(f_Y(Z) = Y\\) to be the ‘coordinate’ functions of \\(Z\\). Then define \\(f(Z) = f_X(Z) + f_Y(Z) = X + Y\\). Now apply LOTUS to \\(f(Z)\\).\n\\[\\E[(X + Y)] = \\E[f(Z)] = \\E[f_X(Z) + f_Y(Z)] = \\E[f_X(Z)] + \\E[f_Y(Z)] = \\E[X] + \\E[Y]\\]\nA somewhat remarkable phenomenon is that it is often easier to compute expectations of the number of times a complex event occurs than the actual probabilities of occurrences. We demonstrate this by the famous “hat problem”.\n\nSuppose \\(n\\) people at a restaurant check their coats. The coat checker is a bit absent-minded and returns the coats to the diners uniformly randomly. On average, how many people return home with their own coat?\n\nMore mathematically, if we shuffle the sequence \\((1, \\dots, n)\\) to generate \\((\\sigma_1, \\sigma_2, \\dots, \\sigma_n)\\), what can we say about the number of times \\(\\sigma_k = k\\)? Specifically, let \\(C\\) be the number of values \\(k\\) such that \\(\\sigma_k = k\\). Computing the distribution of \\(C\\) is somewhat difficult (and a good exercise), but computing \\(\\E[C]\\) is quite easy!\nLet \\(A_k\\) be the event \\(\\sigma_k = k\\): that is, diner \\(k\\) leaves with the proper coat. Clearly \\(C = \\sum_k 1_{A_k}\\), so \\[\\E[C] = \\E\\left[\\sum_{k=1}^n 1_{A_k}\\right] = \\sum_{k=1}^n \\E[1_{A_k}] = \\sum_{k=1}^n P(A_k) = n P(A_1)\\] Because the probabilities are uniform (by assumption), we can write the last step as \\(n P(A_1)\\). Finally, we note that \\(P(A_1)\\) is just the probability that the first person randomly gets the proper coat, which is \\(1/n\\), so we have\n\\[\\E[C] = 1\\]\nThat is, no matter how many coats are checked, on average exactly one person gets the right coat at the end of the evening.\nCompare this to the exact distribution of \\(C\\). Even the ‘simplest’ part - no one getting their own coat - has a difficult probability: \\[\\P(C = 0) = \\sum_{j=0}^n \\frac{(-1)^j}{j!} \\buildrel{n \\to \\infty}\\over\\to\\frac{1}{e} \\approx 36.8\\%\\] And that’s just one term of the sum!"
  },
  {
    "objectID": "notes/expectations.html#official-course-description",
    "href": "notes/expectations.html#official-course-description",
    "title": "What to Expect when You’re Expecting: Notes on Expectations, Variances, and Probabiblities",
    "section": "Official Course Description",
    "text": "Official Course Description\nThis course provides a comprehensive introduction to applied probability and probability distributions. Students will learn probability with an understanding of its applications in statistical inference. Topics include discrete and continuous random variables and distributions, such as the binomial, negative binomial, Poisson, geometric, uniform, normal, exponential, gamma (\\(\\Gamma\\)), beta (\\(B\\)), chi-square (\\(\\chi^2\\)), \\(t\\), and \\(F\\) distributions. This course thoroughly develops topics as transformation of variables, joint distributions, bivariate normal, expectations, conditional distributions and expectations, moment-generating functions, distribution of sums of random variables, means and variances of sums, ratios of independent variables, and central limit theorem. Students will acquire an excellent background to proceed to statistical inference."
  },
  {
    "objectID": "notes/expectations.html#course-learning-objectives",
    "href": "notes/expectations.html#course-learning-objectives",
    "title": "What to Expect when You’re Expecting: Notes on Expectations, Variances, and Probabiblities",
    "section": "Course Learning Objectives",
    "text": "Course Learning Objectives\nStudents successfully completing STA 9715 will:\n\nDefine and manipulate probability mass and density, expectation, and variance for discrete and continuous random variables.\nDefine and manipulate conditional probability, expectation, and variance.\nDefine and manipulate important named probability distributions including, but not limited to, the normal distribution, uniform distribution, and \\(t\\)-distribution.\nManipulate collections of random variables, characterizing them in terms of covariance and correlation, and establishing properties of their limits and sums.\nDefine and manipulate concentration of measure phenomena including the central limit theorem.\n\nAdditionally, STA 9715 will review important mathematical concepts used elsewhere in the statistics curriculum.\nThe following course elements contribute to these goals:\n\nContribution of Course Elements to Learning Goals\n\n\n\n\n\n\n\n\n\n\nPractice Problems\nLearning Goal 1\nLearning Goal 2\nLearning Goal 3\nLearning Goal 4\nLearning Goal 5\n\n\n\n\nWeek 1\n✓\n\n\n\n\n\n\nWeek 2\n✓\n✓\n\n\n\n\n\nWeek 3\n✓\n\n✓\n\n\n\n\nWeek 4\n✓\n\n✓\n\n\n\n\nWeek 5\n✓\n✓\n✓\n\n\n\n\nWeek 6\n✓\n✓\n✓\n✓\n\n\n\nWeek 7\n✓\n✓\n✓\n✓\n\n\n\nWeek 8\n✓\n✓\n✓\n✓\n\n\n\nWeek 10\n✓\n✓\n✓\n✓\n✓\n\n\nWeek 11\n✓\n✓\n✓\n✓\n✓\n\n\nWeek 12\n✓\n✓\n✓\n✓\n✓\n\n\nWeek 13\n✓\n✓\n✓\n✓\n✓"
  },
  {
    "objectID": "notes/expectations.html#program-learning-goals",
    "href": "notes/expectations.html#program-learning-goals",
    "title": "What to Expect when You’re Expecting: Notes on Expectations, Variances, and Probabiblities",
    "section": "Program Learning Goals",
    "text": "Program Learning Goals\nThis course contributes to the program learning goals of several MS programs offered by the Zicklin School of Business.\n\nMS in Business Analytics\nThis course contributes to the following Program Learning Goals for the MS in Business Analytics:\n\nMSBA Program Learning Goals\n\n\n\n\n\n\n\nSTA 9715 Learning Goal\nMSBA Learning Goal\nDescription\n\n\n\n\n\nData Management\nStudents will be able to apply methods, tools, and software for acquiring, managing/storing, and accessing structured and unstructured data. Students will also demonstrate knowledge of the strategic uses of data.\n\n\n✓\nFoundational Statistical / Quantitative Skills\nStudents will be able to prepare data for statistical analysis, perform basic exploratory and descriptive analysis as well as employ foundational statistical techniques needed to analyze data.\n\n\n✓\nAdvanced Statistical/Quantitative Skills\nStudents will be able to build and interpret advanced predictive models. Students will be able to combine business rules and mathematical models to optimize business decisions from data.\n\n\n✓\nEthical Awareness\nStudents will be able to articulate an understanding of ethical issues in all phases of business analytics with particular emphasis on the new possibilities afforded by the emergence of big data.\n\n\n✓\nProfessional Communication\nStudents will be able to explain complex analytical models and their results orally and in writing to technical and non technical/lay audiences.\n\n\n\nKnowledge Integration\nStudents will be able to apply the three key types of analytics (descriptive, predictive, and prescriptive) in a business domain to add value to business decision-making.\n\n\n\n\n\nMS in Quantitative Methods & Modeling\nThis course contributes to the following Program Learning Goals for the MS in Quantitative Methods & Modeling:\n\nMSQMM Program Learning Goals\n\n\n\n\n\n\n\nSTA 9715 Learning Goal\nMSQMM Learning Goal\nDescription\n\n\n\n\n✓\nOperations Research & Mathematical Modeling\nStudents will be able to effectively model, evaluate, and solve quantitative (business) problems using quantitative modeling methods (e.g. deterministic and probabilistic operations research techniques).\n\n\n✓\nStatistics\nStudents will be able to correctly apply appropriate statistical methods when defining, solving, and analyzing problems.\n\n\n\nTechnology Competency\nStudents will be able to use current technological tools, including spreadsheets and specialized software, when solving problems.\n\n\n✓\nProfessional Communication\nStudents will be able to effectively communicate their problem solving methods and solutions to technical and non-technical audiences.\n\n\n\n\n\nMS in Statistics\nThis course contributes to the following Program Learning Goals for the MS in Statistics:\n\nMS Statistics Program Learning Goals\n\n\n\n\n\n\n\nSTA 9715 Learning Goal\nMS Stat Learning Goal\nDescription\n\n\n\n\n✓\nGeneral Statistical Competence\nStudents will be able to apply appropriate probability models and statistical techniques when analyzing problems frm business and other fields.\n\n\n✓\nStatistical Practice\nStudents will become familiar with the standard tools of statistical practice for multiple regression, along with the tools of a subset of specialized statistical areas such as multivariate analysis, applied sampling, time series analysis, experimental design, data mining, categorical analysis, and/or stochastic processes.\n\n\n\nTechnology Competency\nStudents will learn to use one or more of the benchmark statistical software platforms, such as SAS or R."
  },
  {
    "objectID": "notes/expectations.html#footnotes",
    "href": "notes/expectations.html#footnotes",
    "title": "What to Expect when You’re Expecting: Notes on Expectations, Variances, and Probabiblities",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI would encourage you not too focus too much on this distinction: the magic of calculus is that integrals are just infinite limits of very finely diced sums and the same principles apply here. The branch of mathematics that formalizes this connection is measure theory, but we will just assert that these are fungible constructions in this course.↩︎\nThis is, up to sign, essentially the same as the economists’ notion of utility. You may ponder why statisticians work in terms of ‘pain minimization’ instead of ‘happiness maximization’.↩︎\nAs we will see, this is actually quite loose and we can use \\(\\V[X] \\leq \\frac{1}{4}\\) for indicators, but sometimes \\(1\\) makes the math nicer.↩︎\nWe don’t really do complex numbers in this course, but strictly speaking, we should be multiplying by \\(a\\overline{a}\\), which is always positive, even for complex numbers.↩︎"
  },
  {
    "objectID": "notes/expectations.html#transformations-of-random-variables",
    "href": "notes/expectations.html#transformations-of-random-variables",
    "title": "What to Expect when You’re Expecting: Notes on Expectations, Variances, and Probabiblities",
    "section": "Transformations of Random Variables",
    "text": "Transformations of Random Variables\nOften, we may have a random variable \\(X\\) and are interested in performing some calculations on an induced random variable \\(f(X)\\) for some known function \\(f\\). If \\(f\\) is one-to-one, this is easy:\n\\[\\P(f(X) = a) = \\P(X = f^{-1}(a))\\].\nFor example, using our definition of \\(X\\) above,\n\\[ \\P(X^2 = 25) = \\P(X = 5) = \\frac{1}{16}.\\]\nIn other contexts, \\(f\\) may not be one-to-one; that is, there may be multiple \\(x\\) such that \\(f(x) = a\\). In this case, we need to interpret \\(f^{-1}(a)\\) as the set of all inputs leading to \\(a\\). When we compute \\(\\P(X \\in f^{-1}(a))\\), we must compute all possibilities for this input: e.g., let’s take \\(f(x) = (x - 3)^2\\) and compute \\(\\P(f(X) = 1)\\).\n\\[\\P(f(X) = 1) = \\P(X \\in f^{-1}(1)) = \\P(X \\in \\{2, 4\\}) = \\P(X = 2) + \\P(X = 4) = \\frac{1}{2}\\]\nWe have to do a bit more work here to compute \\(f^{-1}(1)\\), but after that, we’re just getting the aggregate probability of disjoint events, so addition suffices. Again - no unions or intersections (inclusion-exclusion rules) in sight: random variables are always in one and only one of their outcomes."
  },
  {
    "objectID": "notes/expectations.html#indicators-and-sums-of-expectations",
    "href": "notes/expectations.html#indicators-and-sums-of-expectations",
    "title": "What to Expect when You’re Expecting: Notes on Expectations, Variances, and Probabiblities",
    "section": "Indicators and Sums of Expectations",
    "text": "Indicators and Sums of Expectations\nA particularly important function class we might consider are indicator functions. For any event \\(A\\), define \\(1_{A}(X)\\) as the function taking \\(1\\) if outcome \\(X\\) is in event \\(A\\) and \\(0\\) otherwise. For example, if \\(A\\) is the set of even numbers, \\(1_A(3) = 0\\) while \\(1_A(2) = 1\\). There is a deep connection between probabilities and expectations of indicator functions.\n\\[\\begin{align*}\n\\E[1_A(X)] &= 0 * \\P(X \\notin A) + 1 * \\P(X \\in A) \\\\\n&= \\P(X \\in A) \\\\\n&= \\P(A)\n\\end{align*}\\]\nBecause of this, any results we have about expectations can be used to derive similar results for probabilities. In particular, we know from statistical theory that sample means converge to expectations: this, in turn, implies that sample probabilities converge to true probabilities. We’ll discuss this below after we talk about variances."
  },
  {
    "objectID": "notes/expectations.html#variances",
    "href": "notes/expectations.html#variances",
    "title": "What to Expect when You’re Expecting: Notes on Expectations, Variances, and Probabiblities",
    "section": "Variances",
    "text": "Variances\nNow that we have a notion of expectations as best predictions, we might ask how wrong our predictions will be ‘on average’. Naively, we might try to compute \\(\\E[(X - \\mu)]\\) for some guess \\(\\mu = \\E[X]\\). Unfortunately, this turns out to be a little useless:\n\\[\\E[(X - \\E[X])] = \\E[X] - \\E[\\E[X]] = \\E[X] - \\E[X] = 0\\]\nBut of course! On average, the average isn’t too high or too low. It’s unbiased in statistical speak. But we still don’t have an answer to our question. Clearly, if we want to know how far off we are, we need to just measure (unsigned) loss. For reasons we discussed earlier, let’s use the squared error loss:\n\\[\\E[\\text{Loss}] = \\E[(X - \\E[X])^2] = \\V[X]\\]\nThis quantity is called the variance of \\(X\\).\n\n\n\n\n\n\nVariance\n\n\n\nThe variance of a random variable \\(X\\) is given by\n\\[\\V[X] = \\E[(X - \\E[X])^2] = \\E[X^2] - \\E[X]^2\\]\n\n\nThe latter equality follows from standard algebra:\n\\[\\begin{align*}\n\\V[X] &= \\E[(X - \\E[X])^2] \\\\\n      &= \\E[X^2 - 2X \\E[X] + \\E[X]^2] \\\\\n      &= \\E[X^2] - 2\\E[X \\E[X]] + \\E[\\E[X]^2] \\\\\n      &= \\E[X^2] - 2\\E[X]\\E[X] + \\E[X]^2 \\\\\n      &= \\E[X^2] - 2\\E[X]^2 + \\E[X]^2 \\\\\n      &= \\E[X^2] - \\E[X]^2\n\\end{align*}\\]\nReading through this, you might find yourself a bit turned around by all of the \\(\\E[\\E[X]X]\\) trickery. While you can justify each step using our properties of expectation listed above, it’s often easier to just let \\(\\mu=\\E[X]\\) to emphasize that, for purposes of a variance calculation, the mean is essentially just any old number. Repeating the above:\n\\[\\begin{align*}\n\\V[X] &= \\E[(X - \\mu)^2] \\\\\n      &= \\E[X^2 - 2X \\mu + \\mu^2] \\\\\n      &= \\E[X^2] - 2\\E[X *\\mu] + \\E[\\mu^2] \\\\\n      &= \\E[X^2] - 2\\mu\\E[X] + \\mu^2 \\\\\n      &= \\E[X^2] - 2\\mu * \\mu + \\mu^2 \\\\\n      &= \\E[X^2] - 2\\mu^2 + \\mu^2\\\\\n      &= \\E[X^2] - \\mu^2\n\\end{align*}\\]\nReturning to the definition of \\(\\V\\), we recall that it is the expectation of a non-negative quantity (a square). Let \\(E_2 = (X - \\E[X])^2\\). Then \\(\\V[X] = \\E[E_2]\\). Since \\(\\min\\{E_2\\} = 0\\), we have \\(\\V[X] \\geq 0\\).\nThis is our first key property fo variances so we should restate it clearly:\n\nFor any random variable \\(X\\), \\(\\V[X] \\geq 0\\). Furthermore \\(\\V[X] = 0\\) only when \\(X\\) is a constant (degenerate) random variable.\n\nLooking more closely at \\(\\V[X] = \\E[E_2]\\), we get the basic intepretation of mean and variance:\n\n\\(\\E[X]\\) is our best squared error prediction of \\(X\\) (point prediction)\nGiven that, \\(\\V[X]\\) is the error we expect to get from our point prediction\n\nFor our next key property, we’ll use the other side of our basic bounds. Suppose \\(X\\) is a random variable that is never more than \\(M\\) in absolute value, \\(\\max\\{|X\\} \\leq M\\). Then \\(\\V[X] \\leq M^2\\). This is a bit crude, but actually surprisingly useful.\nFor instance, let \\(X\\) be an indicator function, taking only values \\(0, 1\\); then \\(\\V[X] \\leq 1\\). Because of this, for any theorem about expectations, we can convert it to a probability result setting variance to 1.3\nWe said earlier that \\(\\E[\\cdot]\\) behaves nicely under linear operators (multiplication and addition): how does \\(\\V[\\cdot]\\) do?\n\\[\\begin{align*}\n\\V[aX] &= \\E[( aX - \\E[aX])^2] \\\\\n       &= \\E[(aX)^2 - 2(aX)\\E[aX] + \\E[aX]^2] \\\\\n       &= a^2 \\left(\\E[X^2 - 2X \\E[X] + \\E[X]^2]\\right) \\\\\n       &= a^2 \\V[X]\n\\end{align*}\\]\nExercise: Using a similar calculation, show \\(\\V[X + b] = \\V[X]\\).\nPutting these together, we have:\n\\[\\V[aX + b] = a^2\\V[X].\\]\nSo addition does nothing, while scalar multiplication applies quadratically. We can justify these from first principles as well:\n\nIf we shift the whole distribution by \\(b\\), our best guess will shift by \\(b\\) as well. But the problem isn’t any harder, so the variance (expected loss) doesn’t change.\nIf we change the units by a factor of \\(a\\) (e.g., feet to inches), our error multiplies by \\(a\\) and so our squared error multiples by \\(a^2\\).\n\nNote that the above formula works even if \\(a &lt; 0\\) because \\(a^2 &gt; 0\\). If we didn’t square things, we could get a negative variance by accident.4\nHow do variances behave with sums of random variables? E.g., \\(\\V[X + Y]\\). Sadly, the answer is far less clear:\n\\[\\begin{align*}\n\\V[X + Y] &= \\E[( (X + Y) - \\E[X+Y] )^2] \\\\\n          &= \\E[((X - \\E[X]) + (Y - \\E[Y]))^2] \\\\\n          &= \\E[(X-\\E[X])^2] + 2\\E[(X - \\E[X])(Y - \\E[Y])] + \\E[(Y - \\E[Y])]^2\n          &= \\V[X] + \\V[Y] + 2\\E[(X - \\E[X])(Y - \\E[Y])]\n\\end{align*}\\]\nDealing that final term, \\(2\\E[(X - \\E[X])(Y - \\E[Y])]\\), will be the subject of our next discussion of covariance and correlation.\n\nInfinite Variances\nFor some distributions, it is possible that \\(\\V[Y]\\) is infinite. These distributions are sometimes called ‘heavy-tailed’. We will discuss these more in a future set of notes, but in brief, these are the distributions that ‘break’ standard statistics."
  },
  {
    "objectID": "notes/expectations.html#conditional-expectations-and-variances",
    "href": "notes/expectations.html#conditional-expectations-and-variances",
    "title": "What to Expect when You’re Expecting: Notes on Expectations, Variances, and Probabiblities",
    "section": "Conditional Expectations and Variances",
    "text": "Conditional Expectations and Variances\nSee textbook."
  }
]